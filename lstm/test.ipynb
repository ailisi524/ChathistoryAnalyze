{"cells":[{"cell_type":"code","source":["!pip install keras"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kglhj2J2rTuy","executionInfo":{"status":"ok","timestamp":1712814479198,"user_tz":-480,"elapsed":8301,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"d1256164-6356-4880-fbc3-63208b72877c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"]}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oULwZRhAsxae","executionInfo":{"status":"ok","timestamp":1712814519668,"user_tz":-480,"elapsed":7499,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"825807df-6dfe-4317-dbeb-c7987cd062d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":["!pip install jieba==0.38"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQhQF1Ogs3oK","executionInfo":{"status":"ok","timestamp":1712814556855,"user_tz":-480,"elapsed":17638,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"a85bf16c-8df4-427a-db5e-ca77f3bec440"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting jieba==0.38\n","  Downloading jieba-0.38.zip (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: jieba\n","  Building wheel for jieba (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jieba: filename=jieba-0.38-py3-none-any.whl size=7361437 sha256=9ec06fdb633092e0b6e34bfe1a08bd557b46de6bd622808614059e73620609fd\n","  Stored in directory: /root/.cache/pip/wheels/ed/12/bb/dafef5e2839e3b563b6cd37686caff82f364ac985fe0f1d757\n","Successfully built jieba\n","Installing collected packages: jieba\n","  Attempting uninstall: jieba\n","    Found existing installation: jieba 0.42.1\n","    Uninstalling jieba-0.42.1:\n","      Successfully uninstalled jieba-0.42.1\n","Successfully installed jieba-0.38\n"]}]},{"cell_type":"code","source":["from sklearn import linear_model"],"metadata":{"id":"cMaUUNZ5tuea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDkhUIcRrVpO","executionInfo":{"status":"ok","timestamp":1712814156435,"user_tz":-480,"elapsed":29805,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"930d9942-a475-4ca6-a712-70eabe0c52de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tk9cT1b8rKFh"},"outputs":[],"source":["import pandas as pd\n","\n","neg = pd.read_csv('/content/drive/MyDrive/iss/NLP Project/SentimentAnalysis-master/data/neg.csv', header=None, index_col=None)\n","pos = pd.read_csv('/content/drive/MyDrive/iss/NLP Project/SentimentAnalysis-master/data/pos.csv', header=None, index_col=None, on_bad_lines='skip')\n","neu = pd.read_csv('/content/drive/MyDrive/iss/NLP Project/SentimentAnalysis-master/data/neutral.csv', header=None, index_col=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UyTYSryrKFl","executionInfo":{"status":"ok","timestamp":1712821930688,"user_tz":-480,"elapsed":529,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"f07562ab-4fdf-4c3d-9037-cbad9c1d8ea8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index([0], dtype='int64')"]},"metadata":{},"execution_count":44}],"source":["neu.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEht8hFmrKFm","executionInfo":{"status":"ok","timestamp":1712815107581,"user_tz":-480,"elapsed":464,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"1d455755-4ef6-455a-82b4-5544803115f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(21088,)"]},"metadata":{},"execution_count":13}],"source":["import numpy as np\n","\n","combined = np.concatenate((pos[0], neu[0], neg[0]))\n","combined.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C04Jl4V1rKFm","executionInfo":{"status":"ok","timestamp":1712815119006,"user_tz":-480,"elapsed":570,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"9093dd37-1843-474e-a3b9-a4b29ce3e3b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(21088,)"]},"metadata":{},"execution_count":14}],"source":["# pos -> 1; neu -> 0; neg -> -1\n","y = np.concatenate((np.ones(len(pos), dtype=int), np.zeros(len(neu), dtype=int), -1*np.ones(len(neg),dtype=int)))\n","y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"MUyHfhVJrKFn","executionInfo":{"status":"ok","timestamp":1712815136978,"user_tz":-480,"elapsed":13339,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"20410cb4-49a3-4f18-9243-569606c22b92"},"outputs":[{"output_type":"stream","name":"stderr","text":["Building prefix dict from the default dictionary ...\n","DEBUG:jieba:Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.928 seconds.\n","DEBUG:jieba:Loading model cost 0.928 seconds.\n","Prefix dict has been built succesfully.\n","DEBUG:jieba:Prefix dict has been built succesfully.\n"]}],"source":["import jieba\n","\n","#对句子经行分词，并去掉换行符\n","def tokenizer(text):\n","    ''' Simple Parser converting each document to lower-case, then\n","        removing the breaks for new lines and finally splitting on the\n","        whitespace\n","    '''\n","    text = [jieba.lcut(document.replace('\\n', '')) for document in text]\n","    return text\n","\n","combined = tokenizer(combined)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yyj42IoorKFo","executionInfo":{"status":"ok","timestamp":1712815881577,"user_tz":-480,"elapsed":18673,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"d8d5a6ea-c064-40a3-88a6-b193b87cebec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training a Word2vec model...\n"]}],"source":["from gensim.models.word2vec import Word2Vec\n","from gensim.corpora.dictionary import Dictionary\n","from keras.preprocessing import sequence\n","import multiprocessing\n","\n","cpu_count = multiprocessing.cpu_count() # 4\n","vocab_dim = 100\n","n_iterations = 10  # ideally more..   epoch\n","n_exposures = 10 # 所有频数超过10的词语\n","window_size = 7\n","n_epoch = 4\n","input_length = 100\n","maxlen = 100\n","\n","def create_dictionaries(model=None, combined=None):\n","    if (combined is not None) and (model is not None):\n","        gensim_dict = Dictionary()\n","        # 使用 model.wv.key_to_index 替代 model.vocab.keys()\n","        gensim_dict.doc2bow(model.wv.key_to_index.keys(),\n","                            allow_update=True)\n","        # 由于 model.wv.key_to_index 已经是 {word: index} 的映射，所以可以直接使用\n","        w2indx = {word: index + 1 for word, index in model.wv.key_to_index.items()}  # 词语的索引\n","        w2vec = {word: model.wv[word] for word in w2indx.keys()}  # 词语的词向量\n","\n","        def parse_dataset(combined):  # 闭包-->临时使用\n","            data = []\n","            for sentence in combined:\n","                new_txt = []\n","                for word in sentence:\n","                    try:\n","                        new_txt.append(w2indx[word])\n","                    except:\n","                        new_txt.append(0)  # 词频小于10的词语索引为0\n","                data.append(new_txt)\n","            return data  # word => index\n","\n","        combined = parse_dataset(combined)\n","        combined = sequence.pad_sequences(combined, maxlen=maxlen)  # 对句子进行填充\n","        return w2indx, w2vec, combined\n","    else:\n","        print('No data provided...')\n","\n","\n","#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n","def word2vec_train(combined):\n","\n","    model = Word2Vec(vector_size=vocab_dim,\n","                     min_count=n_exposures,\n","                     window=window_size,\n","                     workers=cpu_count,\n","                     epochs=n_iterations)\n","    model.build_vocab(combined) # input: list\n","    #model.train(combined)\n","    model.train(combined, total_examples=model.corpus_count, epochs=model.epochs)\n","    model.save('/content/drive/MyDrive/iss/NLP Project/SentimentAnalysis-master/model/Word2vec_model.pkl')\n","    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n","    return   index_dict, word_vectors,combined\n","\n","print ('Training a Word2vec model...')\n","index_dict, word_vectors,combined=word2vec_train(combined)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ol4HaviOrKFp","executionInfo":{"status":"ok","timestamp":1712817320552,"user_tz":-480,"elapsed":272287,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"5bdbe3c9-1c0f-404f-e3d7-db3d0d2c3633"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting up Arrays for Keras Embedding Layer...\n","x_train.shape and y_train.shape:\n","(16870, 100) (16870, 3)\n","Defining a Simple Keras Model...\n","Compiling the Model...\n","Train...\n","Epoch 1/4\n","528/528 [==============================] - 56s 101ms/step - loss: 0.7553 - accuracy: 0.6780\n","Epoch 2/4\n","528/528 [==============================] - 55s 105ms/step - loss: 0.3823 - accuracy: 0.8780\n","Epoch 3/4\n","528/528 [==============================] - 55s 105ms/step - loss: 0.3458 - accuracy: 0.8947\n","Epoch 4/4\n","528/528 [==============================] - 54s 103ms/step - loss: 0.2545 - accuracy: 0.9224\n","Evaluate...\n","132/132 [==============================] - 3s 20ms/step - loss: 0.2943 - accuracy: 0.9059\n","Test score: [0.29425284266471863, 0.905879557132721]\n"]}],"source":["from sklearn.model_selection import train_test_split  # 更新导入路径\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense, Dropout, Activation  # 更新了导入的层\n","from keras.models import model_from_yaml\n","import numpy as np\n","import sys\n","sys.setrecursionlimit(1000000)\n","import yaml\n","import keras\n","\n","batch_size = 32\n","\n","\n","def get_data(index_dict,word_vectors,combined,y):\n","\n","    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n","    embedding_weights = np.zeros((n_symbols, vocab_dim)) # 初始化 索引为0的词语，词向量全为0\n","    for word, index in index_dict.items(): # 从索引为1的词语开始，对每个词语对应其词向量\n","        embedding_weights[index, :] = word_vectors[word]\n","    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n","    y_train = keras.utils.to_categorical(y_train,num_classes=3)\n","    y_test = keras.utils.to_categorical(y_test,num_classes=3)\n","    # print x_train.shape,y_train.shape\n","    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test\n","\n","\n","##定义网络结构\n","def train_lstm(n_symbols, embedding_weights, x_train, y_train, x_test, y_test):\n","    print('Defining a Simple Keras Model...')\n","    model = Sequential()\n","    model.add(Embedding(output_dim=vocab_dim,\n","                        input_dim=n_symbols,\n","                        mask_zero=True,\n","                        weights=[embedding_weights],\n","                        input_length=input_length))\n","    model.add(LSTM(units=50, activation='tanh', recurrent_activation='hard_sigmoid'))  # 更新参数名\n","    model.add(Dropout(0.5))\n","    model.add(Dense(units=3, activation='softmax'))  # 明确指定 units 参数\n","    # Activation 层在这里可能是多余的，因为上一层 Dense 已经使用了 'softmax' 激活函数\n","\n","    print('Compiling the Model...')\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam', metrics=['accuracy'])\n","\n","    print(\"Train...\")\n","    model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch, verbose=1)\n","\n","    print(\"Evaluate...\")\n","    score = model.evaluate(x_test, y_test, batch_size=batch_size)\n","\n","    # 使用 model.to_json() 替换 model.to_yaml()\n","    json_string = model.to_json()\n","    with open('/content/drive/MyDrive/iss/NLP Project/SentimentAnalysis-master/model/lstm.json', 'w') as outfile:\n","        outfile.write(json_string)\n","    # 继续保存你的模型权重\n","    model.save_weights('/content/drive/MyDrive/iss/NLP Project/SentimentAnalysis-master/model/lstm.h5')\n","    print('Test score:', score)\n","\n","print ('Setting up Arrays for Keras Embedding Layer...')\n","n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, word_vectors,combined,y)\n","print (\"x_train.shape and y_train.shape:\")\n","print (x_train.shape,y_train.shape)\n","train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ApYXml0ErKFq"},"outputs":[],"source":["\"\"\"\n","预测\n","\"\"\"\n","import jieba\n","import numpy as np\n","from gensim.models.word2vec import Word2Vec\n","from gensim.corpora.dictionary import Dictionary\n","from keras.preprocessing import sequence\n","\n","\n","from keras.models import model_from_json\n","np.random.seed(1337)  # For Reproducibility\n","import sys\n","sys.setrecursionlimit(1000000)\n","\n","# define parameters\n","maxlen = 100\n","\n","def create_dictionaries(model=None, combined=None):\n","    if (combined is not None) and (model is not None):\n","        gensim_dict = Dictionary()\n","        # 使用 model.wv.key_to_index 替代 model.vocab.keys()\n","        gensim_dict.doc2bow(model.wv.key_to_index.keys(),\n","                            allow_update=True)\n","        # 由于 model.wv.key_to_index 已经是 {word: index} 的映射，所以可以直接使用\n","        w2indx = {word: index + 1 for word, index in model.wv.key_to_index.items()}  # 词语的索引\n","        w2vec = {word: model.wv[word] for word in w2indx.keys()}  # 词语的词向量\n","\n","        def parse_dataset(combined):  # 闭包-->临时使用\n","            data = []\n","            for sentence in combined:\n","                new_txt = []\n","                for word in sentence:\n","                    try:\n","                        new_txt.append(w2indx[word])\n","                    except:\n","                        new_txt.append(0)  # 词频小于10的词语索引为0\n","                data.append(new_txt)\n","            return data  # word => index\n","        combined = parse_dataset(combined)\n","        combined = sequence.pad_sequences(combined, maxlen=maxlen)  # 对句子进行填充\n","        return w2indx, w2vec, combined\n","    else:\n","        print('No data provided...')\n","\n","\n","def input_transform(string):\n","    words=jieba.lcut(string)\n","    words=np.array(words).reshape(1,-1)\n","    model=Word2Vec.load('/content/drive/MyDrive/iss/NLP Project/SentimentAnalysis-master/model/Word2vec_model.pkl')\n","    _,_,combined=create_dictionaries(model,words)\n","    return combined\n","\n","\n","def lstm_predict(string):\n","    print('loading model......')\n","\n","    with open('/content/drive/MyDrive/iss/NLP Project/SentimentAnalysis-master/model/lstm.json', 'r') as f:\n","        json_string = f.read()\n","    model = model_from_json(json_string)\n","\n","    print('loading weights......')\n","    model.load_weights('/content/drive/MyDrive/iss/NLP Project/SentimentAnalysis-master/model/lstm.h5')\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam', metrics=['accuracy'])\n","    data = input_transform(string)\n","    data = data.reshape(1, -1)  # 注意这里需要用赋值的方式来更新 data 的形状\n","    # print(data)\n","\n","    # 使用 predict 方法来替代 predict_classes 方法\n","    result = model.predict(data)\n","    predicted_class = np.argmax(result, axis=1)  # 获取最高概率的类别索引\n","    print(predicted_class)  # 输出预测的类别索引，比如 [1]\n","\n","    if predicted_class[0] == 1:\n","        print(string, 'positive')\n","    elif predicted_class[0] == 0:\n","        print(string, 'neutral')\n","    else:\n","        print(string, 'negative')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUSl1lAErKFq","executionInfo":{"status":"ok","timestamp":1712818996309,"user_tz":-480,"elapsed":1105,"user":{"displayName":"Yifei Huang","userId":"03611263398327237760"}},"outputId":"632eb117-5a7d-48b7-d685-56401d75fcc6"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading model......\n","loading weights......\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7b383a07dab0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 257ms/step\n","[1]\n","你真的太好了啊 positive\n"]}],"source":["# string='酒店的环境非常好，价格也便宜，值得推荐'\n","# string='手机质量太差了，傻逼店家，赚黑心钱，以后再也不会买了'\n","# string = \"这是我看过文字写得很糟糕的书，因为买了，还是耐着性子看完了，但是总体来说不好，文字、内容、结构都不好\"\n","# string = \"虽说是职场指导书，但是写的有点干涩，我读一半就看不下去了！\"\n","# string = \"书的质量还好，但是内容实在没意思。本以为会侧重心理方面的分析，但实际上是婚外恋内容。\"\n","# string = \"不是太好\"\n","# string = \"不错不错\"\n","string = \"你真的太好了啊\"\n","# string = \"真的一般，没什么可以学习的\"\n","\n","lstm_predict(string)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"S1qw-U1orKFr"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}